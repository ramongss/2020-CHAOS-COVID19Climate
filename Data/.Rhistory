apply(ptest[1:(n-cut),6:7],2,function(x){mape(data_Close[(cut+4):length(data_Close)],x)})
data_plot<-data.frame(INDEX=rep(seq(1,n-cut,1),times=3),
STOCK=unlist(data_Close[(cut+4):length(data_Close)],ptest[1:(n-cut),6:7]),
TYPE=rep(c("Obs","CEEMD+XGB","CEEMD+XGB+RIDGE"),each=366,times=3))
X11()
ggplot(data_plot, aes(x = INDEX, y = STOCK,colour=TYPE)) +
geom_line() +
xlab("Index") +  ylab("STOCK(R$)")+
theme_bw(base_size = 18)+
theme(#legend.position = "bottom",
legend.direction = "horizontal",
legend.title = element_blank(),
legend.position = c(0.35, 0.91),
legend.background = element_rect(linetype = 2, size = 0.5, colour = 1),
legend.text = element_text(size=20),
axis.text=element_text(size=17),
axis.title=element_text(size=20))
for(i in 1:dim(data_final)[2])
{
#--------------------------Construindo Conjuno-----------------------------#
data_dec[[i]]<-lags(data_final[,i], n = 3) #n representa o número de lags
colnames(data_dec[[i]])<-c(names(data_final[i]),paste("Lag",1:3,sep=""))
#----------------------Divisão em treinamento e teste---------------------#
n      <-dim(data_dec[[i]])[1] #Número de observações
cut    <-round(0.7*n,0)        #Ponto de corte entre treino e teste
train  <-data_dec[[i]][1:cut,];
Y_train<-train[,1];X_train<-train[,-1]
test   <-tail(data_dec[[i]],n-cut)
Y_test <-test[,1]; X_test <-test[,-1]
#----------------------Divisões para Treinamento----------------------------#
fitControl<- trainControl(method         = "none") #Sem validação cruzada
#Treinando e prevendo cada componente com cada modelo
X_trainm<-rbind(train[,-1],Aux);
X_testm <-rbind(test[,-1],Aux);
colnames(X_trainm)=colnames(X_train)
colnames(X_testm) =colnames(X_test)
#-----------------------Treinamento----------------------------------------#
set.seed(1234)
Models[[k]]<-train(as.data.frame(X_trainm[1:cut,]),as.numeric(Y_train),
method="rf",
preProcess = c("center","scale"), #Processamento Centro e Escala
#tuneLength= 5,                    #Número de tipos de parâmetros
trControl = fitControl,verbose=FALSE)
#-----------------------Salvando Parâmetros--------------------------------#
Params[[k]]<-Models[[k]]$bestTune
#---------------------------1SA-Predições-----------------------------------------#
Lag1<-match("Lag1",colnames(X_testm))
Lag2<-match("Lag2",colnames(X_testm))
Lag3<-match("Lag3",colnames(X_testm))
#------------------Recursive Forecasting for train and test sets-----------
#Aqui, o conjunto de análise é dividido em n conjuntos de h observações. Nesse caso
#A cada 3 predições, é reiniciado o processo de previsão. Caso isso não seja feito,
#as predições continuarão a ser atualizadas e o erro carregado.
#Se desejar h>3, basta tirar fazer H<-HORIZONTE DESEJADO e descomentar
#X_trainm[p+3,Lag3]<-ptrain[p,m] e #X_testm[p+3,Lag3]<-ptest[p,m]
h<-3
#Train
for(p in 1:cut)
{
if(p%%h !=1) #Sempre reinicia na divisão de resto 1-->Multiplos de h+1
{
ptrain[p,i]<-as.numeric(predict(Models[[k]], as.data.frame(t(X_trainm[p,]))))
X_trainm[p+1,Lag1]<-ptrain[p,i]
X_trainm[p+2,Lag2]<-ptrain[p,i]
#X_trainm[p+3,Lag3]<-ptrain[p,i]
}
else
{
X_trainm[p:(n-cut),]<-X_train[p:(n-cut),]
ptrain[p,i]       <-as.numeric(predict(Models[[k]], as.data.frame(t(X_trainm[p,]))))
X_trainm[p+1,Lag1]<-ptrain[p,i]
X_trainm[p+2,Lag2]<-ptrain[p,i]
#X_trainm[p+3,Lag3]<-ptrain[p,i]
}
}
#Test
for(p in 1:(n-cut))
{
if(p%%h !=1)
{
ptest[p,i]<-as.numeric(predict(Models[[k]],as.data.frame(t(X_testm[p,]))))
X_testm[p+1,Lag1]<-ptest[p,i]
X_testm[p+2,Lag2]<-ptest[p,i]
#X_testm[p+3,Lag3]<-ptest[p,i]
}
else
{
X_testm[p:(n-cut),]<-X_test[p:(n-cut),]
ptest[p,i]       <-as.numeric(predict(Models[[k]],as.data.frame(t(X_testm[p,]))))
X_testm[p+1,Lag1]<-ptest[p,i]
X_testm[p+2,Lag2]<-ptest[p,i]
#X_testm[p+3,Lag3]<-ptest[p,i]
}
}
k<-k+1
cat("Component:",i,"ok \n")
}
ptrain[1:cut,dim(data_final)[2]+1]   <-apply(ptrain[1:cut,1:dim(data_final)[2]],1,sum)
ptest[1:(n-cut),dim(data_final)[2]+1]<-apply(ptest[1:(n-cut),1:dim(data_final)[2]],1,sum)
set.seed(0)
train_control = trainControl(method = 'cv', number = 10)
grid = 10 ^ seq(5, -2, length = 100)
tune.grid = expand.grid(lambda = grid, alpha = 0)
ridge.caret = train(data.frame(ptrain[1:cut,1:5]), as.numeric(Y_train),
method = 'glmnet',
trControl = train_control,
tuneGrid = tune.grid)
ridge.caret$bestTune
ridge_full <- train(data.frame(ptrain[1:cut,1:5]), as.numeric(Y_train),
method = 'glmnet',
trControl = trainControl(method = 'none'),
tuneGrid = expand.grid(
lambda = ridge.caret$bestTune$lambda, alpha = 0)
)
coefs<-coef(ridge_full$finalModel, s = ridge.caret$bestTune$lambda)
ptrain[1:cut,dim(data_final)[2]+2]   <-coefs[1]+coefs[2]*ptrain[1:cut,1]+coefs[3]*ptrain[1:cut,2]+
coefs[4]*ptrain[1:cut,3]+coefs[5]*ptrain[1:cut,4]+coefs[6]*ptrain[1:cut,5]
ptest[1:(n-cut),dim(data_final)[2]+2]<-coefs[1]+coefs[2]*ptest[1:(n-cut),1]+coefs[3]*ptest[1:(n-cut),2]+
coefs[4]*ptest[1:(n-cut),3]+coefs[5]*ptest[1:(n-cut),4]+coefs[6]*ptest[1:(n-cut),5]
apply(ptrain[1:cut,6:7],2,function(x){cor(data_Close[4:(cut+3)],x)^2})
apply(ptest[1:(n-cut),6:7],2,function(x){cor(data_Close[(cut+4):length(data_Close)],x)^2})
#RMSE
apply(ptrain[1:cut,6:7],2,function(x){mape(data_Close[4:(cut+3)],x)})
apply(ptest[1:(n-cut),6:7],2,function(x){mape(data_Close[(cut+4):length(data_Close)],x)})
for(i in 1:dim(data_final)[2])
{
#--------------------------Construindo Conjuno-----------------------------#
data_dec[[i]]<-lags(data_final[,i], n = 3) #n representa o número de lags
colnames(data_dec[[i]])<-c(names(data_final[i]),paste("Lag",1:3,sep=""))
#----------------------Divisão em treinamento e teste---------------------#
n      <-dim(data_dec[[i]])[1] #Número de observações
cut    <-round(0.7*n,0)        #Ponto de corte entre treino e teste
train  <-data_dec[[i]][1:cut,];
Y_train<-train[,1];X_train<-train[,-1]
test   <-tail(data_dec[[i]],n-cut)
Y_test <-test[,1]; X_test <-test[,-1]
#----------------------Divisões para Treinamento----------------------------#
fitControl<- trainControl(method         = "none") #Sem validação cruzada
#Treinando e prevendo cada componente com cada modelo
X_trainm<-rbind(train[,-1],Aux);
X_testm <-rbind(test[,-1],Aux);
colnames(X_trainm)=colnames(X_train)
colnames(X_testm) =colnames(X_test)
#-----------------------Treinamento----------------------------------------#
set.seed(1234)
Models[[k]]<-train(as.data.frame(X_trainm[1:cut,]),as.numeric(Y_train),
method="brnn",
preProcess = c("center","scale"), #Processamento Centro e Escala
#tuneLength= 5,                    #Número de tipos de parâmetros
trControl = fitControl,verbose=FALSE)
#-----------------------Salvando Parâmetros--------------------------------#
Params[[k]]<-Models[[k]]$bestTune
#---------------------------1SA-Predições-----------------------------------------#
Lag1<-match("Lag1",colnames(X_testm))
Lag2<-match("Lag2",colnames(X_testm))
Lag3<-match("Lag3",colnames(X_testm))
#------------------Recursive Forecasting for train and test sets-----------
#Aqui, o conjunto de análise é dividido em n conjuntos de h observações. Nesse caso
#A cada 3 predições, é reiniciado o processo de previsão. Caso isso não seja feito,
#as predições continuarão a ser atualizadas e o erro carregado.
#Se desejar h>3, basta tirar fazer H<-HORIZONTE DESEJADO e descomentar
#X_trainm[p+3,Lag3]<-ptrain[p,m] e #X_testm[p+3,Lag3]<-ptest[p,m]
h<-3
#Train
for(p in 1:cut)
{
if(p%%h !=1) #Sempre reinicia na divisão de resto 1-->Multiplos de h+1
{
ptrain[p,i]<-as.numeric(predict(Models[[k]], as.data.frame(t(X_trainm[p,]))))
X_trainm[p+1,Lag1]<-ptrain[p,i]
X_trainm[p+2,Lag2]<-ptrain[p,i]
#X_trainm[p+3,Lag3]<-ptrain[p,i]
}
else
{
X_trainm[p:(n-cut),]<-X_train[p:(n-cut),]
ptrain[p,i]       <-as.numeric(predict(Models[[k]], as.data.frame(t(X_trainm[p,]))))
X_trainm[p+1,Lag1]<-ptrain[p,i]
X_trainm[p+2,Lag2]<-ptrain[p,i]
#X_trainm[p+3,Lag3]<-ptrain[p,i]
}
}
#Test
for(p in 1:(n-cut))
{
if(p%%h !=1)
{
ptest[p,i]<-as.numeric(predict(Models[[k]],as.data.frame(t(X_testm[p,]))))
X_testm[p+1,Lag1]<-ptest[p,i]
X_testm[p+2,Lag2]<-ptest[p,i]
#X_testm[p+3,Lag3]<-ptest[p,i]
}
else
{
X_testm[p:(n-cut),]<-X_test[p:(n-cut),]
ptest[p,i]       <-as.numeric(predict(Models[[k]],as.data.frame(t(X_testm[p,]))))
X_testm[p+1,Lag1]<-ptest[p,i]
X_testm[p+2,Lag2]<-ptest[p,i]
#X_testm[p+3,Lag3]<-ptest[p,i]
}
}
k<-k+1
cat("Component:",i,"ok \n")
}
ptrain[1:cut,dim(data_final)[2]+1]   <-apply(ptrain[1:cut,1:dim(data_final)[2]],1,sum)
ptest[1:(n-cut),dim(data_final)[2]+1]<-apply(ptest[1:(n-cut),1:dim(data_final)[2]],1,sum)
set.seed(0)
train_control = trainControl(method = 'cv', number = 10)
grid = 10 ^ seq(5, -2, length = 100)
tune.grid = expand.grid(lambda = grid, alpha = 0)
ridge.caret = train(data.frame(ptrain[1:cut,1:5]), as.numeric(Y_train),
method = 'glmnet',
trControl = train_control,
tuneGrid = tune.grid)
ridge.caret$bestTune
ridge_full <- train(data.frame(ptrain[1:cut,1:5]), as.numeric(Y_train),
method = 'glmnet',
trControl = trainControl(method = 'none'),
tuneGrid = expand.grid(
lambda = ridge.caret$bestTune$lambda, alpha = 0)
)
coefs<-coef(ridge_full$finalModel, s = ridge.caret$bestTune$lambda)
ptrain[1:cut,dim(data_final)[2]+2]   <-coefs[1]+coefs[2]*ptrain[1:cut,1]+coefs[3]*ptrain[1:cut,2]+
coefs[4]*ptrain[1:cut,3]+coefs[5]*ptrain[1:cut,4]+coefs[6]*ptrain[1:cut,5]
ptest[1:(n-cut),dim(data_final)[2]+2]<-coefs[1]+coefs[2]*ptest[1:(n-cut),1]+coefs[3]*ptest[1:(n-cut),2]+
coefs[4]*ptest[1:(n-cut),3]+coefs[5]*ptest[1:(n-cut),4]+coefs[6]*ptest[1:(n-cut),5]
#R2 para treino e teste
apply(ptrain[1:cut,6:7],2,function(x){cor(data_Close[4:(cut+3)],x)^2})
apply(ptest[1:(n-cut),6:7],2,function(x){cor(data_Close[(cut+4):length(data_Close)],x)^2})
#RMSE
apply(ptrain[1:cut,6:7],2,function(x){mape(data_Close[4:(cut+3)],x)})
apply(ptest[1:(n-cut),6:7],2,function(x){mape(data_Close[(cut+4):length(data_Close)],x)})
for(i in 1:dim(data_final)[2])
{
#--------------------------Construindo Conjuno-----------------------------#
data_dec[[i]]<-lags(data_final[,i], n = 3) #n representa o número de lags
colnames(data_dec[[i]])<-c(names(data_final[i]),paste("Lag",1:3,sep=""))
#----------------------Divisão em treinamento e teste---------------------#
n      <-dim(data_dec[[i]])[1] #Número de observações
cut    <-round(0.7*n,0)        #Ponto de corte entre treino e teste
train  <-data_dec[[i]][1:cut,];
Y_train<-train[,1];X_train<-train[,-1]
test   <-tail(data_dec[[i]],n-cut)
Y_test <-test[,1]; X_test <-test[,-1]
#----------------------Divisões para Treinamento----------------------------#
fitControl<- trainControl(method         = "none") #Sem validação cruzada
#Treinando e prevendo cada componente com cada modelo
X_trainm<-rbind(train[,-1],Aux);
X_testm <-rbind(test[,-1],Aux);
colnames(X_trainm)=colnames(X_train)
colnames(X_testm) =colnames(X_test)
#-----------------------Treinamento----------------------------------------#
set.seed(1234)
Models[[k]]<-train(as.data.frame(X_trainm[1:cut,]),as.numeric(Y_train),
method="xgbTree",
preProcess = c("center","scale"), #Processamento Centro e Escala
#tuneLength= 5,                    #Número de tipos de parâmetros
trControl = fitControl,verbose=FALSE)
#-----------------------Salvando Parâmetros--------------------------------#
Params[[k]]<-Models[[k]]$bestTune
#---------------------------1SA-Predições-----------------------------------------#
Lag1<-match("Lag1",colnames(X_testm))
Lag2<-match("Lag2",colnames(X_testm))
Lag3<-match("Lag3",colnames(X_testm))
#------------------Recursive Forecasting for train and test sets-----------
#Aqui, o conjunto de análise é dividido em n conjuntos de h observações. Nesse caso
#A cada 3 predições, é reiniciado o processo de previsão. Caso isso não seja feito,
#as predições continuarão a ser atualizadas e o erro carregado.
#Se desejar h>3, basta tirar fazer H<-HORIZONTE DESEJADO e descomentar
#X_trainm[p+3,Lag3]<-ptrain[p,m] e #X_testm[p+3,Lag3]<-ptest[p,m]
h<-3
#Train
for(p in 1:cut)
{
if(p%%h !=1) #Sempre reinicia na divisão de resto 1-->Multiplos de h+1
{
ptrain[p,i]<-as.numeric(predict(Models[[k]], as.data.frame(t(X_trainm[p,]))))
X_trainm[p+1,Lag1]<-ptrain[p,i]
X_trainm[p+2,Lag2]<-ptrain[p,i]
#X_trainm[p+3,Lag3]<-ptrain[p,i]
}
else
{
X_trainm[p:(n-cut),]<-X_train[p:(n-cut),]
ptrain[p,i]       <-as.numeric(predict(Models[[k]], as.data.frame(t(X_trainm[p,]))))
X_trainm[p+1,Lag1]<-ptrain[p,i]
X_trainm[p+2,Lag2]<-ptrain[p,i]
#X_trainm[p+3,Lag3]<-ptrain[p,i]
}
}
#Test
for(p in 1:(n-cut))
{
if(p%%h !=1)
{
ptest[p,i]<-as.numeric(predict(Models[[k]],as.data.frame(t(X_testm[p,]))))
X_testm[p+1,Lag1]<-ptest[p,i]
X_testm[p+2,Lag2]<-ptest[p,i]
#X_testm[p+3,Lag3]<-ptest[p,i]
}
else
{
X_testm[p:(n-cut),]<-X_test[p:(n-cut),]
ptest[p,i]       <-as.numeric(predict(Models[[k]],as.data.frame(t(X_testm[p,]))))
X_testm[p+1,Lag1]<-ptest[p,i]
X_testm[p+2,Lag2]<-ptest[p,i]
#X_testm[p+3,Lag3]<-ptest[p,i]
}
}
k<-k+1
cat("Component:",i,"ok \n")
}
ptrain[1:cut,dim(data_final)[2]+1]   <-apply(ptrain[1:cut,1:dim(data_final)[2]],1,sum)
ptest[1:(n-cut),dim(data_final)[2]+1]<-apply(ptest[1:(n-cut),1:dim(data_final)[2]],1,sum)
set.seed(0)
train_control = trainControl(method = 'cv', number = 10)
grid = 10 ^ seq(5, -2, length = 100)
tune.grid = expand.grid(lambda = grid, alpha = 0)
ridge.caret = train(data.frame(ptrain[1:cut,1:5]), as.numeric(Y_train),
method = 'glmnet',
trControl = train_control,
tuneGrid = tune.grid)
ridge.caret$bestTune
ridge_full <- train(data.frame(ptrain[1:cut,1:5]), as.numeric(Y_train),
method = 'glmnet',
trControl = trainControl(method = 'none'),
tuneGrid = expand.grid(
lambda = ridge.caret$bestTune$lambda, alpha = 0)
)
coefs<-coef(ridge_full$finalModel, s = ridge.caret$bestTune$lambda)
ptrain[1:cut,dim(data_final)[2]+2]   <-coefs[1]+coefs[2]*ptrain[1:cut,1]+coefs[3]*ptrain[1:cut,2]+
coefs[4]*ptrain[1:cut,3]+coefs[5]*ptrain[1:cut,4]+coefs[6]*ptrain[1:cut,5]
ptest[1:(n-cut),dim(data_final)[2]+2]<-coefs[1]+coefs[2]*ptest[1:(n-cut),1]+coefs[3]*ptest[1:(n-cut),2]+
coefs[4]*ptest[1:(n-cut),3]+coefs[5]*ptest[1:(n-cut),4]+coefs[6]*ptest[1:(n-cut),5]
#R2 para treino e teste
apply(ptrain[1:cut,6:7],2,function(x){cor(data_Close[4:(cut+3)],x)^2})
apply(ptest[1:(n-cut),6:7],2,function(x){cor(data_Close[(cut+4):length(data_Close)],x)^2})
#RMSE
apply(ptrain[1:cut,6:7],2,function(x){mape(data_Close[4:(cut+3)],x)})
apply(ptest[1:(n-cut),6:7],2,function(x){mape(data_Close[(cut+4):length(data_Close)],x)})
for(i in 1:dim(data_final)[2])
{
#--------------------------Construindo Conjuno-----------------------------#
data_dec[[i]]<-lags(data_final[,i], n = 3) #n representa o número de lags
colnames(data_dec[[i]])<-c(names(data_final[i]),paste("Lag",1:3,sep=""))
#----------------------Divisão em treinamento e teste---------------------#
n      <-dim(data_dec[[i]])[1] #Número de observações
cut    <-round(0.7*n,0)        #Ponto de corte entre treino e teste
train  <-data_dec[[i]][1:cut,];
Y_train<-train[,1];X_train<-train[,-1]
test   <-tail(data_dec[[i]],n-cut)
Y_test <-test[,1]; X_test <-test[,-1]
#----------------------Divisões para Treinamento----------------------------#
fitControl<- trainControl(method         = "none") #Sem validação cruzada
#Treinando e prevendo cada componente com cada modelo
X_trainm<-rbind(train[,-1],Aux);
X_testm <-rbind(test[,-1],Aux);
colnames(X_trainm)=colnames(X_train)
colnames(X_testm) =colnames(X_test)
#-----------------------Treinamento----------------------------------------#
set.seed(1234)
Models[[k]]<-train(as.data.frame(X_trainm[1:cut,]),as.numeric(Y_train),
method="xgbLinear",
preProcess = c("center","scale"), #Processamento Centro e Escala
#tuneLength= 5,                    #Número de tipos de parâmetros
trControl = fitControl,verbose=FALSE)
#-----------------------Salvando Parâmetros--------------------------------#
Params[[k]]<-Models[[k]]$bestTune
#---------------------------1SA-Predições-----------------------------------------#
Lag1<-match("Lag1",colnames(X_testm))
Lag2<-match("Lag2",colnames(X_testm))
Lag3<-match("Lag3",colnames(X_testm))
#------------------Recursive Forecasting for train and test sets-----------
#Aqui, o conjunto de análise é dividido em n conjuntos de h observações. Nesse caso
#A cada 3 predições, é reiniciado o processo de previsão. Caso isso não seja feito,
#as predições continuarão a ser atualizadas e o erro carregado.
#Se desejar h>3, basta tirar fazer H<-HORIZONTE DESEJADO e descomentar
#X_trainm[p+3,Lag3]<-ptrain[p,m] e #X_testm[p+3,Lag3]<-ptest[p,m]
h<-3
#Train
for(p in 1:cut)
{
if(p%%h !=1) #Sempre reinicia na divisão de resto 1-->Multiplos de h+1
{
ptrain[p,i]<-as.numeric(predict(Models[[k]], as.data.frame(t(X_trainm[p,]))))
X_trainm[p+1,Lag1]<-ptrain[p,i]
X_trainm[p+2,Lag2]<-ptrain[p,i]
#X_trainm[p+3,Lag3]<-ptrain[p,i]
}
else
{
X_trainm[p:(n-cut),]<-X_train[p:(n-cut),]
ptrain[p,i]       <-as.numeric(predict(Models[[k]], as.data.frame(t(X_trainm[p,]))))
X_trainm[p+1,Lag1]<-ptrain[p,i]
X_trainm[p+2,Lag2]<-ptrain[p,i]
#X_trainm[p+3,Lag3]<-ptrain[p,i]
}
}
#Test
for(p in 1:(n-cut))
{
if(p%%h !=1)
{
ptest[p,i]<-as.numeric(predict(Models[[k]],as.data.frame(t(X_testm[p,]))))
X_testm[p+1,Lag1]<-ptest[p,i]
X_testm[p+2,Lag2]<-ptest[p,i]
#X_testm[p+3,Lag3]<-ptest[p,i]
}
else
{
X_testm[p:(n-cut),]<-X_test[p:(n-cut),]
ptest[p,i]       <-as.numeric(predict(Models[[k]],as.data.frame(t(X_testm[p,]))))
X_testm[p+1,Lag1]<-ptest[p,i]
X_testm[p+2,Lag2]<-ptest[p,i]
#X_testm[p+3,Lag3]<-ptest[p,i]
}
}
k<-k+1
cat("Component:",i,"ok \n")
}
ptrain[1:cut,dim(data_final)[2]+1]   <-apply(ptrain[1:cut,1:dim(data_final)[2]],1,sum)
ptest[1:(n-cut),dim(data_final)[2]+1]<-apply(ptest[1:(n-cut),1:dim(data_final)[2]],1,sum)
set.seed(0)
train_control = trainControl(method = 'cv', number = 10)
grid = 10 ^ seq(5, -2, length = 100)
tune.grid = expand.grid(lambda = grid, alpha = 0)
ridge.caret = train(data.frame(ptrain[1:cut,1:5]), as.numeric(Y_train),
method = 'glmnet',
trControl = train_control,
tuneGrid = tune.grid)
ridge.caret$bestTune
ridge_full <- train(data.frame(ptrain[1:cut,1:5]), as.numeric(Y_train),
method = 'glmnet',
trControl = trainControl(method = 'none'),
tuneGrid = expand.grid(
lambda = ridge.caret$bestTune$lambda, alpha = 0)
)
coefs<-coef(ridge_full$finalModel, s = ridge.caret$bestTune$lambda)
ptrain[1:cut,dim(data_final)[2]+2]   <-coefs[1]+coefs[2]*ptrain[1:cut,1]+coefs[3]*ptrain[1:cut,2]+
coefs[4]*ptrain[1:cut,3]+coefs[5]*ptrain[1:cut,4]+coefs[6]*ptrain[1:cut,5]
ptest[1:(n-cut),dim(data_final)[2]+2]<-coefs[1]+coefs[2]*ptest[1:(n-cut),1]+coefs[3]*ptest[1:(n-cut),2]+
coefs[4]*ptest[1:(n-cut),3]+coefs[5]*ptest[1:(n-cut),4]+coefs[6]*ptest[1:(n-cut),5]
#R2 para treino e teste
apply(ptrain[1:cut,6:7],2,function(x){cor(data_Close[4:(cut+3)],x)^2})
apply(ptest[1:(n-cut),6:7],2,function(x){cor(data_Close[(cut+4):length(data_Close)],x)^2})
#RMSE
apply(ptrain[1:cut,6:7],2,function(x){mape(data_Close[4:(cut+3)],x)})
apply(ptest[1:(n-cut),6:7],2,function(x){mape(data_Close[(cut+4):length(data_Close)],x)})
data_plot<-data.frame(INDEX=rep(seq(1,n-cut,1),times=2),
STOCK=unlist(data_Close[(cut+4):length(data_Close)],ptest[1:(n-cut),7]),
TYPE=rep(c("Obs","CEEMD+XGB+RIDGE"),each=366,times=2))
X11()
ggplot(data_plot, aes(x = INDEX, y = STOCK,colour=TYPE)) +
geom_line() +
xlab("Index") +  ylab("STOCK(R$)")+
theme_bw(base_size = 18)+
theme(#legend.position = "bottom",
legend.direction = "horizontal",
legend.title = element_blank(),
legend.position = c(0.35, 0.91),
legend.background = element_rect(linetype = 2, size = 0.5, colour = 1),
legend.text = element_text(size=20),
axis.text=element_text(size=17),
axis.title=element_text(size=20))
library(gamlss)
install.packages("gamlss")
library(gamlss)
dat1<-rBE(100, mu=.3, sigma=.5)
hist(dat1)
mod1<-gamlss(dat1~1,family=BE) # fits a constant for mu and sigma
str(data1)
str(dat1)
mod1<-gamlss(dat1~1,family=BE) # fits a constant for mu and sigma
fitted(mod1)[1]
fitted(mod1,"sigma")[1]
rm(list = ls())
#setwd("C:/Users/Usuario/Dropbox/COVID-19")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#Save several directories
BaseDir       <- getwd()
CodesDir      <- paste(BaseDir, "Codes", sep="/")
FiguresDir    <- paste(BaseDir, "Figures", sep="/")
ResultsDir    <- paste(BaseDir, "Results", sep="/")
DataDir       <- paste(BaseDir, "Data",sep="/")
#Load Packages
setwd(CodesDir)
source("checkpackages.R")
source("elm_caret.R")
source("Metricas.R")
#library(devtools)
#install_github('lhmet/inmetr')
packages<-c("forecast","cowplot","Metrics","caret","elmNNRcpp","tcltk","TTR",
"foreach","iterators","doParallel","lmtest","wmtsa","httr",
"jsonlite","magrittr","xlsx","devtools","inmetr")
sapply(packages,packs)
library(extrafont)
windowsFonts(Times = windowsFont("TT Times New Roman"))
library(ggplot2)
library(Cairo)
#Checa quantos núcleos existem
ncl<-detectCores();ncl
#Registra os clusters a serem utilizados
cl <- makeCluster(ncl-1);registerDoParallel(cl)
setwd(DataDir)
dados_SP     <-read.table(file="climate_SaoPaulo_2020-04-30.csv"  ,header=TRUE,sep=";",dec=",")
dados_SP     <-read.table(file="climate_SaoPaulo_2020-04-30.csv"  ,header=TRUE,sep=";",dec=".")
str(dados_SP)
dados_SP     <-read.table(file="climate_SaoPaulo_2020-04-30.csv"  ,header=TRUE,sep=";",dec=",")
str(dados_SP)
dados_CE     <-read.table(file="climate_Ceara_2020-04-30.csv"     ,header=TRUE,sep=";",dec=".")
str(dados_C)
str(dados_CE)
